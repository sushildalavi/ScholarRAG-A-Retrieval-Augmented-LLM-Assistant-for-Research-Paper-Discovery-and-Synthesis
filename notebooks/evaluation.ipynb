{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ScholarRAG Evaluation Notebook\n",
        "\n",
        "Use this notebook to inspect retrieval quality and LLM-judge scores.\n",
        "\n",
        "Inputs:\n",
        "\n",
        "- `runs/ask_results.jsonl`: generated by `evaluation/run_batch.py`\n",
        "- `runs/ask_results_scored.jsonl`: same file after passing through `evaluation/llm_judge.py`\n",
        "\n",
        "Update the paths below if you store results elsewhere."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "Generate runs/ask_results.jsonl via evaluation/run_batch.py first.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m JUDGED_FILE = RUN_DIR / \u001b[33m'\u001b[39m\u001b[33mask_results_scored.jsonl\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m RAW_FILE.exists():\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mGenerate runs/ask_results.jsonl via evaluation/run_batch.py first.\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: Generate runs/ask_results.jsonl via evaluation/run_batch.py first."
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# Configure paths\n",
        "RUN_DIR = Path('../runs')\n",
        "RAW_FILE = RUN_DIR / 'ask_results.jsonl'\n",
        "JUDGED_FILE = RUN_DIR / 'ask_results_scored.jsonl'\n",
        "\n",
        "if not RAW_FILE.exists():\n",
        "    raise FileNotFoundError('Generate runs/ask_results.jsonl via evaluation/run_batch.py first.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_jsonl(path: Path):\n",
        "    with path.open() as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                yield json.loads(line)\n",
        "\n",
        "records = list(load_jsonl(RAW_FILE))\n",
        "len(records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.json_normalize(records, record_path='retrieved', meta=['query', 'k', 'year_from', 'year_to', 'relevant_ids'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieval Metrics\n",
        "\n",
        "Assumes each query object in the input file includes `relevant_ids` listing known good paper identifiers (e.g. OpenAlex IDs or DOIs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relevance_flag(row):\n",
        "    rel = row.get('relevant_ids') or []\n",
        "    identifiers = [str(row.get('openalex_id')), str(row.get('doi'))]\n",
        "    return any(i and i in rel for i in identifiers)\n",
        "\n",
        "df['is_relevant'] = df.apply(relevance_flag, axis=1)\n",
        "df['rank'] = df['rank'].astype(int)\n",
        "df.sample(min(3, len(df)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def precision_at_k(group, k=10):\n",
        "    topk = group.nsmallest(k, 'rank')\n",
        "    if topk.empty:\n",
        "        return np.nan\n",
        "    return topk['is_relevant'].mean()\n",
        "\n",
        "def recall_at_k(group, k=10):\n",
        "    rel_total = len(group['relevant_ids'].iloc[0] or [])\n",
        "    if rel_total == 0:\n",
        "        return np.nan\n",
        "    topk = group.nsmallest(k, 'rank')\n",
        "    return topk['is_relevant'].sum() / rel_total\n",
        "\n",
        "rows = []\n",
        "for k_val in (3, 5, 10):\n",
        "    precision = df.groupby('query').apply(precision_at_k, k=k_val).mean()\n",
        "    recall = df.groupby('query').apply(recall_at_k, k=k_val).mean()\n",
        "    rows.append({'k': k_val, 'precision_at_k': precision, 'recall_at_k': recall})\n",
        "\n",
        "pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLM Judge Scores (optional)\n",
        "\n",
        "Run `evaluation/llm_judge.py --input runs/ask_results.jsonl --output runs/ask_results_scored.jsonl` first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if JUDGED_FILE.exists():\n",
        "    judged_records = list(load_jsonl(JUDGED_FILE))\n",
        "    judged_df = pd.json_normalize(judged_records)\n",
        "    judged_df[['query', 'evaluation.score', 'evaluation.verdict']].head()\n",
        "else:\n",
        "    judged_df = None\n",
        "    print('LLM judge file not found. Run evaluation/llm_judge.py to populate it.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if judged_df is not None:\n",
        "    avg_score = judged_df['evaluation.score'].mean()\n",
        "    pass_rate = (judged_df['evaluation.verdict'] == 'pass').mean()\n",
        "    print(f'Average judge score: {avg_score:.2f}')\n",
        "    print(f'Pass rate: {pass_rate*100:.1f}%')\n",
        "    fig = px.histogram(judged_df, x='evaluation.score', nbins=6, title='LLM Judge Score Distribution')\n",
        "    fig.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
